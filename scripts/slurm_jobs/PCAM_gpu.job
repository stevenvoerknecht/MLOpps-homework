#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --gpus-per-task=1
#SBATCH --mem=16G
#SBATCH --partition=gpu_course
#SBATCH --time=00:30:00
#SBATCH --output=job_%j.out
#SBATCH --error=job_%j.err
#Load environment modules
module load 2025
module load Python/3.13.1-GCCcore-14.2.0
module load matplotlib/3.10.3-gfbf-2025a
# Set output directory to /scratch-shared/<user>/data
INPUT_DIR="/scratch-shared/${USER}/data"
# Activate environment
source $HOME/venv/bin/activate
# Copying the data from scratch-shared to local SSD node to make hdf5 faster
echo "Copying data to TMPDIR..."
cp -r /scratch-shared/$USER/data $TMPDIR/data
DATA_DIR="$TMPDIR/data"
echo "Data copied"
# Making an output directory
OUT_DIR="$HOME/MLOps_2026/experiments/$SLURM_JOB_ID"
mkdir -p "$OUT_DIR"
# Execute program located in $HOME, using the environment
# This program might write the pre-processed data to the output dir
python -u $HOME/MLOps_2026/scripts/example_training_loop/training_loop.py "$DATA_DIR" "$OUT_DIR"  